{
  "paragraphs": [
    {
      "text": "%pyspark\ndef is_notebook() -\u003e bool:\n    try:\n        shell \u003d get_ipython().__class__.__name__\n        if shell \u003d\u003d \u0027ZMQInteractiveShell\u0027:\n            return True   # Jupyter notebook or qtconsole\n        elif shell \u003d\u003d \u0027TerminalInteractiveShell\u0027:\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False      # Probably standard Python interpreter",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:50.866",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183833_1918749928",
      "id": "paragraph_1662638585445_500092845",
      "dateCreated": "2022-09-12 11:29:43.833",
      "dateStarted": "2022-09-13 12:09:50.870",
      "dateFinished": "2022-09-13 12:09:51.083",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql import SQLContext, DataFrame, Row, Window\nfrom pyspark.sql import SparkSession\nfrom pyspark.context import SparkContext\nfrom datetime import timedelta, date, datetime\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:51.170",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183834_841047687",
      "id": "20220822-102308_775973632",
      "dateCreated": "2022-09-12 11:29:43.834",
      "dateStarted": "2022-09-13 12:09:51.174",
      "dateFinished": "2022-09-13 12:09:51.388",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\nif is_notebook():\n sys.argv\u003d[\u0027\u0027,\u0027{\"MaintenancePathPrefix\": \"/JUPITER/OUTPUT/#MAINTENANCE/2022-09-09_manual__2022-09-09T11%3A15%3A40%2B00%3A00_\", \"ProcessDate\": \"2022-09-12\", \"Schema\": \"Jupiter\", \"PipelineName\": \"jupiter_shipto_client_mapping\"}\u0027]\n \n sc.addPyFile(\"hdfs:///SRC/SHARED/EXTRACT_SETTING.py\")\n os.environ[\"HADOOP_USER_NAME\"] \u003d \"airflow\"",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:51.474",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183834_616754462",
      "id": "paragraph_1662641609562_245750343",
      "dateCreated": "2022-09-12 11:29:43.834",
      "dateStarted": "2022-09-13 12:09:51.478",
      "dateFinished": "2022-09-13 12:09:51.690",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\nspark \u003d SparkSession.builder.appName(\u0027Jupiter - PySpark\u0027).getOrCreate()\nimport EXTRACT_SETTING as es\n\nSETTING_RAW_DIR \u003d es.SETTING_RAW_DIR\nSETTING_PROCESS_DIR \u003d es.SETTING_PROCESS_DIR\nSETTING_OUTPUT_DIR \u003d es.SETTING_OUTPUT_DIR\n\nDATE_DIR\u003des.DATE_DIR\n\nEXTRACT_ENTITIES_AUTO_PATH \u003d f\u0027{es.HDFS_PREFIX}{es.MAINTENANCE_PATH_PREFIX}EXTRACT_ENTITIES_AUTO.csv\u0027\nprocessDate\u003des.processDate\npipelineRunId\u003des.pipelineRunId\n\nprint(f\u0027EXTRACT_ENTITIES_AUTO_PATH\u003d{EXTRACT_ENTITIES_AUTO_PATH}\u0027)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:51.778",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "EXTRACT_ENTITIES_AUTO_PATH\u003dhdfs:///JUPITER/OUTPUT/#MAINTENANCE/2022-09-09_manual__2022-09-09T11%3A15%3A40%2B00%3A00_EXTRACT_ENTITIES_AUTO.csv\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183834_385020815",
      "id": "paragraph_1662641706132_932546591",
      "dateCreated": "2022-09-12 11:29:43.834",
      "dateStarted": "2022-09-13 12:09:51.782",
      "dateFinished": "2022-09-13 12:09:52.001",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\nCUSTOMER_ATTR_DATA \u003d SETTING_RAW_DIR + \"/SOURCES/HYDRATEATLAS/0CUSTOMER_ATTR/DATA/\"\nCUSTOMER_TEXT_DATA \u003d SETTING_RAW_DIR + \"/SOURCES/HYDRATEATLAS/0CUSTOMER_TEXT/DATA/\"\nCUST_SALES_ATTR_DATA \u003d SETTING_RAW_DIR + \"/SOURCES/HYDRATEATLAS/0CUST_SALES_ATTR/DATA/\"\nCUST_SALES_TEXT_DATA \u003d SETTING_RAW_DIR + \"/SOURCES/HYDRATEATLAS/0CUST_SALES_TEXT/DATA/\"\nCUST_HIER \u003d SETTING_RAW_DIR + \"/SOURCES/HYDRATEATLAS/0CUST_SALES_LKDH_HIER_T_ELEMENTS/DATA/\"\nZCUSTOPF_ATTR \u003d SETTING_RAW_DIR + \"/SOURCES/HYDRATEATLAS/ZCUSTOPF_ATTR/DATA/\"\n\nRESULT_MAPPING_PARQ \u003d SETTING_PROCESS_DIR + \"/SHIPTO_TO_CLIENT_MAPPING/SHIPTO_TO_CLIENT_MAPPING.PARQUET\"",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:52.083",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183834_1476706042",
      "id": "20220822-102308_380969602",
      "dateCreated": "2022-09-12 11:29:43.834",
      "dateStarted": "2022-09-13 12:09:52.087",
      "dateFinished": "2022-09-13 12:09:52.299",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\ncustHierSchema \u003d StructType([StructField(\"HIENM\", StringType(), False),\n                              StructField(\"HCLASS\", StringType(), False),\n                              StructField(\"DATEFROM\", StringType(), False),\n                              StructField(\"DATETO\", StringType(), False),\n                              StructField(\"HEADERID\", StringType(), False),\n                              StructField(\"NODEID\", StringType(), False),\n                              StructField(\"IOBJNM\", StringType(), False),\n                              StructField(\"NODENAME\", StringType(), False),\n                              StructField(\"TLEVEL\", StringType(), False),\n                              StructField(\"LINK\", StringType(), False),\n                              StructField(\"PARENTID\", StringType(), False),\n                              StructField(\"CHILDID\", StringType(), False),\n                              StructField(\"NEXTID\", StringType(), False),\n                              StructField(\"ODQ_CHANGEMODE\", StringType(), False),\n                              StructField(\"ODQ_ENTITYCNTR\", StringType(), False),\n                              StructField(\"LOAD_DATE\", StringType(), False),\n                              StructField(\"LOAD_DATETIME\", StringType(), False),\n                              StructField(\"INPUT_FILE_NAME\", StringType(), False)])\n\ncustopfSchema \u003d StructType([StructField(\u0027KUNNR_PK\u0027, StringType(), False),\n                            StructField(\u0027VKORG_PK\u0027, StringType(), False),\n                            StructField(\u0027VTWEG_PK\u0027, StringType(), False),\n                            StructField(\u0027SPART_PK\u0027, StringType(), False),\n                            StructField(\u0027PARVW_PK\u0027, StringType(), False),\n                            StructField(\u0027PARZA_PK\u0027, StringType(), False),\n                            StructField(\u0027KUNN2_PK\u0027, StringType(), False),\n                            StructField(\u0027KTOKD_PK\u0027, StringType(), False),\n                            StructField(\u0027KNREF_PK\u0027, StringType(), False),\n                            StructField(\u0027ODQ_CHANGEMODE\u0027, StringType(), False),\n                            StructField(\u0027ODQ_ENTITYCNTR\u0027, StringType(), False),\n                            StructField(\u0027LOAD_DATE\u0027, StringType(), False),\n                            StructField(\u0027LOAD_DATETIME\u0027, StringType(), False),\n                            StructField(\u0027INPUT_FILE_NAME\u0027, StringType(), False)])\n\ncustomerTextSchema \u003d StructType([StructField(\u0027KUNNR_PK\u0027 , StringType(), False),\n                              StructField(\u0027TXTMD\u0027 , StringType(), False),\n                              StructField(\u0027TXTLG\u0027 , StringType(), False),\n                              StructField(\u0027ODQ_CHANGEMODE\u0027 , StringType(), False),\n                              StructField(\u0027ODQ_ENTITYCNTR\u0027 , StringType(), False),\n                              StructField(\u0027LOAD_DATE\u0027 , StringType(), False),\n                              StructField(\u0027LOAD_DATETIME\u0027 , StringType(), False),\n                              StructField(\u0027INPUT_FILE_NAME\u0027 , StringType(), False)])\n\ncustHierDf \u003d spark.read.schema(custHierSchema).parquet(CUST_HIER)\ncustomerTextDf \u003d spark.read.schema(customerTextSchema).parquet(CUSTOMER_TEXT_DATA)\ncustopfDf \u003d spark.read.schema(custopfSchema).parquet(ZCUSTOPF_ATTR)\n\ncustHierDf \u003d custHierDf.\\\nwithColumn(\u0027DATEFROM\u0027,to_date(col(\u0027DATEFROM\u0027),\u0027yyyy.MM.dd\u0027)).\\\nwithColumn(\u0027DATETO\u0027,to_date(col(\u0027DATETO\u0027),\u0027yyyy.MM.dd\u0027)).\\\nwithColumn(\u0027TLEVEL\u0027,col(\u0027TLEVEL\u0027).cast(IntegerType()))",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:52.387",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183834_1579053680",
      "id": "20220822-102308_706017728",
      "dateCreated": "2022-09-12 11:29:43.834",
      "dateStarted": "2022-09-13 12:09:52.391",
      "dateFinished": "2022-09-13 12:09:52.653",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\ncustHierDf \u003d custHierDf.select(col(\"NODEID\").alias(\"NODEID\"),\\\n    col(\"IOBJNM\").alias(\"InfoObject(IOBJNM)\"),\\\n    col(\"NODENAME\").alias(\"NODENAME\"),\\\n    col(\"TLEVEL\").alias(\"Le\"),\\\n    col(\"LINK\").alias(\"L\"),\\\n    col(\"PARENTID\").alias(\"Parent ID\"),\\\n    col(\"CHILDID\").alias(\"Child ID\"),\\\n    col(\"NEXTID\").alias(\"Next ID\"),\\\n    col(\"DATEFROM\").alias(\"Active_From\"),\\\n    col(\"DATETO\").alias(\"Active_Till\")).where(col(\"HIENM\")\u003d\u003d\"G\")\n\ncustHierDf \u003d custHierDf.\\\n    withColumn(\u0027H_DIV\u0027,regexp_extract(\u0027NODENAME\u0027, \u0027(^.{0,2})(.{2,2})(.{2,3})\u0027, 1)).\\\n    withColumn(\u0027H_CH\u0027,regexp_extract(\u0027NODENAME\u0027, \u0027(^.{0,2})(.{2,2})(.{2,3})\u0027, 2)).\\\n    withColumn(\u0027H_SO\u0027,regexp_extract(\u0027NODENAME\u0027, \u0027(^.{0,2})(.{2,2})(.{2,3})\u0027, 3)).\\\n    withColumn(\u0027H_PTR\u0027,regexp_extract(\u0027NODENAME\u0027, \u0027(.{10})\\\\s*$\u0027, 1))\ncustHierDf\u003dcustHierDf.where( (col(\u0027H_DIV\u0027) \u003d\u003d \u002751\u0027) \u0026 (col(\u0027H_SO\u0027) \u003d\u003d \u0027261\u0027) \u0026 (col(\u0027H_CH\u0027).isin({\u002733\u0027,\u002722\u0027,\u002711\u0027})))",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:52.691",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183834_2144589053",
      "id": "paragraph_1661240575704_729581714",
      "dateCreated": "2022-09-12 11:29:43.834",
      "dateStarted": "2022-09-13 12:09:52.695",
      "dateFinished": "2022-09-13 12:09:52.958",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\nchildParentDf \u003d custHierDf.select(col(\u0027NODEID\u0027).alias(\u0027NODEID\u0027),col(\u0027Parent ID\u0027).alias(\u0027PARENTID\u0027),col(\u0027H_PTR\u0027).alias(\u0027NODEADDR\u0027))\n\ncustHierLvlPrevDf \u003d custHierDf.join(childParentDf,(custHierDf[\u0027NODEID\u0027]\u003d\u003dchildParentDf.NODEID) \u0026 (custHierDf[\u0027Le\u0027]\u003d\u003d5) | (custHierDf[\u0027NODEID\u0027]\u003d\u003dchildParentDf.NODEID) \u0026 (custHierDf[\u0027Le\u0027]\u003d\u003d4), how\u003d\u0027left\u0027).drop(childParentDf[\u0027NODEID\u0027])\n\ncustHierLvlPrevDf \u003d custHierLvlPrevDf\\\n  .withColumn(\u0027Lvl5\u0027,when(col(\u0027Le\u0027)\u003d\u003d5,col(\u0027NODEID\u0027)))\\\n  .withColumn(\u0027Lvl4\u0027,when(col(\u0027Le\u0027)\u003d\u003d5,col(\u0027PARENTID\u0027))\\\n  .otherwise(when(col(\u0027Le\u0027)\u003d\u003d4,col(\u0027NODEID\u0027))\\\n  .otherwise(None))).drop(\u0027PARENTID\u0027,\u0027NODEADDR\u0027)\n\ncustHierLvlPrevDf \u003d custHierLvlPrevDf.join(childParentDf\\\n                                         ,custHierLvlPrevDf[\u0027Lvl4\u0027]\u003d\u003dchildParentDf.NODEID,how\u003d\u0027left\u0027).drop(childParentDf[\u0027NODEID\u0027])\n\ncustHierLvlPrevDf \u003d custHierLvlPrevDf.withColumnRenamed(\u0027PARENTID\u0027,\u0027lvl4parent\u0027).drop(\u0027NODEADDR\u0027)\n\nfor l in range(4,2,-1):\n    custHierLvlCurDf \u003d custHierLvlPrevDf.join(childParentDf\\\n        ,((custHierLvlPrevDf[\u0027NODEID\u0027]\u003d\u003dchildParentDf.NODEID) \u0026 (custHierLvlPrevDf[\u0027Le\u0027]\u003e\u003d(l))\\\n          |(custHierLvlPrevDf[\u0027NODEID\u0027]\u003d\u003dchildParentDf.NODEID) \u0026 (custHierDf[\u0027Le\u0027]\u003d\u003d(l-1))),how\u003d\u0027left\u0027).drop(childParentDf[\u0027NODEID\u0027])\n    custHierLvlCurDf \u003d custHierLvlCurDf.withColumn(\u0027Lvl\u0027+str(l-1)\\\n                                                 ,when(col(\u0027Le\u0027)\u003e\u003dl,col(\u0027lvl\u0027+str(l)+\u0027parent\u0027))\\\n                                                 .otherwise(when(col(\u0027Le\u0027)\u003d\u003d(l-1),col(\u0027NODEID\u0027))\\\n                                                 .otherwise(None))).drop(\u0027PARENTID\u0027,\u0027NODEADDR\u0027)\n    \n    custHierLvlCurDf \u003d custHierLvlCurDf.join(childParentDf,custHierLvlCurDf[\u0027Lvl\u0027+str(l-1)]\u003d\u003dchildParentDf.NODEID,how\u003d\u0027left\u0027).drop(childParentDf[\u0027NODEID\u0027])\n    custHierLvlCurDf \u003d custHierLvlCurDf.withColumnRenamed(\u0027PARENTID\u0027,\u0027Lvl\u0027+str(l-1)+\u0027parent\u0027).drop(\u0027NODEADDR\u0027)\n    custHierLvlPrevDf \u003d custHierLvlCurDf\n    \ncustHierLvlCurDf \u003d custHierLvlCurDf.drop(\u0027lvl4parent\u0027,\u0027lvl3parent\u0027,\u0027lvl2parent\u0027)    \n  \nfor l in range(2,6):  \n  custHierLvlCurDf \u003d custHierLvlCurDf.join(childParentDf,custHierLvlCurDf[\u0027Lvl\u0027+str(l)]\u003d\u003dchildParentDf[\u0027NODEID\u0027],how\u003d\u0027left\u0027)\n  custHierLvlCurDf \u003d custHierLvlCurDf.withColumnRenamed(\u0027NODEADDR\u0027,\u0027ZCUSTHG0\u0027+str(l-1)).drop(\u0027NODEID\u0027).drop(\u0027PARENTID\u0027).drop(\u0027Lvl\u0027+str(l))",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:52.995",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183834_1469137841",
      "id": "20220822-102308_1631086647",
      "dateCreated": "2022-09-12 11:29:43.834",
      "dateStarted": "2022-09-13 12:09:53.000",
      "dateFinished": "2022-09-13 12:09:53.566",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\ncustHierDupDf \u003d custHierDf.select(col(\u0027NODEID\u0027).alias(\u0027NODE_ID\u0027),col(\u0027H_PTR\u0027).alias(\u0027G_H_ParentID\u0027))\ncustHierLvlCurDf \u003d custHierLvlCurDf.join(custHierDupDf,custHierLvlCurDf[\u0027Parent ID\u0027]\u003d\u003dcustHierDupDf[\u0027NODE_ID\u0027],how\u003d\u0027left\u0027)\ncustHierLevels1to5Df \u003d custHierLvlCurDf\\\n.select(col(\u0027H_PTR\u0027).alias(\u00270CUST_SALES\u0027),\\\n        col(\u0027H_DIV\u0027).alias(\u0027Division\u0027),\\\n        col(\u0027H_SO\u0027).alias(\u00270SALESORG\u0027),\\\n        col(\u0027H_CH\u0027).alias(\u00270DISTR_CHAN\u0027),\\\n        \u0027ZCUSTHG01\u0027,\u0027ZCUSTHG02\u0027,\u0027ZCUSTHG03\u0027,\u0027ZCUSTHG04\u0027,\\\n        col(\u0027Active_From\u0027),col(\u0027Active_Till\u0027),\u0027G_H_ParentID\u0027,col(\u0027Le\u0027).alias(\u0027G_H_level\u0027))\n\ncustHierLevels1to5Df \u003d custHierLevels1to5Df.withColumn(\u0027G_H_level\u0027,col(\u0027G_H_level\u0027) - 1)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:53.600",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183835_1116969238",
      "id": "20220822-102308_1906519523",
      "dateCreated": "2022-09-12 11:29:43.835",
      "dateStarted": "2022-09-13 12:09:53.604",
      "dateFinished": "2022-09-13 12:09:53.867",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\ncustopfZaDf \u003d custopfDf.where( (col(\u0027VKORG_PK\u0027) \u003d\u003d \u0027261\u0027) \\\n                            \u0026 (col(\u0027VTWEG_PK\u0027).isin({\u002733\u0027,\u002722\u0027,\u002711\u0027})) \\\n                            \u0026 col(\u0027PARVW_PK\u0027).isin({\u0027ZA\u0027}) \\\n                            \u0026 (col(\u0027PARZA_PK\u0027) \u003d\u003d \u00270\u0027) \\\n                            \u0026 (col(\u0027KTOKD_PK\u0027).isin(\u00270001\u0027, \u00270002\u0027)) \\\n                            \u0026 (col(\u0027SPART_PK\u0027) \u003d\u003d \u002751\u0027 ))\n\ncustHierLevels1to5PlusShiptoDf \u003d custHierLevels1to5Df.join(custopfZaDf,\\\n                                                         ((custopfZaDf.KUNN2_PK \u003d\u003d custHierLevels1to5Df[\u00270CUST_SALES\u0027]) \\\n                                                          \u0026 (custopfZaDf.VKORG_PK \u003d\u003d custHierLevels1to5Df[\u00270SALESORG\u0027]) \\\n                                                          \u0026 (custopfZaDf.VTWEG_PK \u003d\u003d custHierLevels1to5Df[\u00270DISTR_CHAN\u0027]) \\\n                                                          \u0026 (custopfZaDf.SPART_PK \u003d\u003d custHierLevels1to5Df.Division)),how\u003d\u0027inner\u0027)\n\ncustHierLevels1to5PlusShiptoDf \u003d custHierLevels1to5PlusShiptoDf.withColumn(\u0027G_H_level\u0027,col(\u0027G_H_level\u0027)+1)\\\n    .withColumn(\u0027G_H_ParentID\u0027,col(\u00270CUST_SALES\u0027))\\\n    .withColumn(\u00270CUST_SALES\u0027,col(\u0027KUNNR_PK\u0027))\n\ncustHierLevels1to5PlusShiptoDf \u003d custHierLevels1to5PlusShiptoDf.\\\nselect(\u00270CUST_SALES\u0027,\u0027Division\u0027,\u00270SALESORG\u0027,\u00270DISTR_CHAN\u0027,\u0027ZCUSTHG01\u0027,\\\n       \u0027ZCUSTHG02\u0027,\u0027ZCUSTHG03\u0027,\u0027ZCUSTHG04\u0027,\u0027Active_From\u0027,\u0027Active_Till\u0027,\u0027G_H_ParentID\u0027,\u0027G_H_level\u0027)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:53.905",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183835_2045833126",
      "id": "20220822-102308_1772397072",
      "dateCreated": "2022-09-12 11:29:43.835",
      "dateStarted": "2022-09-13 12:09:53.908",
      "dateFinished": "2022-09-13 12:09:54.221",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\nhydratePlusShiptoDf \u003d custHierLevels1to5Df.union(custHierLevels1to5PlusShiptoDf)\ncustomerFilteredDf \u003d customerTextDf.select(\u0027KUNNR_PK\u0027,\u0027TXTMD\u0027)\\\n        .withColumn(\u0027TXTMD\u0027,regexp_replace(\u0027TXTMD\u0027, \u0027RU$\u0027, \u0027\u0027))\\\n        .withColumn(\u0027TXTMD\u0027,regexp_replace(\u0027TXTMD\u0027, \u0027 $\u0027, \u0027\u0027))\n\nhydratePlusShiptoDf \u003d hydratePlusShiptoDf.join(customerFilteredDf,hydratePlusShiptoDf.ZCUSTHG01\u003d\u003dcustomerFilteredDf.KUNNR_PK,how\u003d\u0027left\u0027)\\\n.withColumnRenamed(\u0027TXTMD\u0027,\u0027ZCUSTHG01___T\u0027).drop(\u0027KUNNR_PK\u0027)\nhydratePlusShiptoDf \u003d hydratePlusShiptoDf.join(customerFilteredDf,hydratePlusShiptoDf.ZCUSTHG02\u003d\u003dcustomerFilteredDf.KUNNR_PK,how\u003d\u0027left\u0027)\\\n.withColumnRenamed(\u0027TXTMD\u0027,\u0027ZCUSTHG02___T\u0027).drop(\u0027KUNNR_PK\u0027)\nhydratePlusShiptoDf \u003d hydratePlusShiptoDf.join(customerFilteredDf,hydratePlusShiptoDf.ZCUSTHG03\u003d\u003dcustomerFilteredDf.KUNNR_PK,how\u003d\u0027left\u0027)\\\n.withColumnRenamed(\u0027TXTMD\u0027,\u0027ZCUSTHG03___T\u0027).drop(\u0027KUNNR_PK\u0027)\nhydratePlusShiptoDf \u003d hydratePlusShiptoDf.join(customerFilteredDf,hydratePlusShiptoDf.ZCUSTHG04\u003d\u003dcustomerFilteredDf.KUNNR_PK,how\u003d\u0027left\u0027)\\\n.withColumnRenamed(\u0027TXTMD\u0027,\u0027ZCUSTHG04___T\u0027).drop(\u0027KUNNR_PK\u0027)\n\nhydratePlusShiptoDf \u003d hydratePlusShiptoDf.join(customerFilteredDf,\\\n                                             hydratePlusShiptoDf[\u00270CUST_SALES\u0027]\u003d\u003dcustomerFilteredDf.KUNNR_PK,how\u003d\u0027left\u0027)\\\n.withColumnRenamed(\u0027TXTMD\u0027,\u00270CUST_SALES___T\u0027).drop(\u0027KUNNR_PK\u0027)\n\nhydratePlusShiptoDf \u003d hydratePlusShiptoDf.\\\nselect(\u00270CUST_SALES\u0027,\u00270CUST_SALES___T\u0027,\u0027Division\u0027,\u00270SALESORG\u0027,\u00270DISTR_CHAN\u0027,\u0027ZCUSTHG01\u0027,\u0027ZCUSTHG01___T\u0027,\u0027ZCUSTHG02\u0027,\u0027ZCUSTHG02___T\u0027,\u0027ZCUSTHG03\u0027,\u0027ZCUSTHG03___T\u0027,\u0027ZCUSTHG04\u0027,\u0027ZCUSTHG04___T\u0027,\u0027Active_From\u0027,\u0027Active_Till\u0027,\u0027G_H_ParentID\u0027,\u0027G_H_level\u0027)\n\nhydratePlusShiptoDf \u003d hydratePlusShiptoDf.where(col(\u0027G_H_level\u0027) \u003d\u003d 5)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:54.309",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183835_1305838171",
      "id": "20220822-102308_1961995961",
      "dateCreated": "2022-09-12 11:29:43.835",
      "dateStarted": "2022-09-13 12:09:54.313",
      "dateFinished": "2022-09-13 12:09:54.777",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\ncustopfSpShDf \u003d custopfDf.where( (col(\u0027VKORG_PK\u0027) \u003d\u003d \u0027261\u0027)\\\n                              \u0026 (col(\u0027VTWEG_PK\u0027).isin({\u002733\u0027,\u002722\u0027,\u002711\u0027})) \\\n                              \u0026 (col(\u0027PARVW_PK\u0027) \u003d\u003d \u0027SH\u0027) \\\n                              \u0026 (col(\u0027SPART_PK\u0027) \u003d\u003d \u002751\u0027) \\\n                              \u0026 (col(\u0027KTOKD_PK\u0027).isin(\u00270001\u0027, \u00270002\u0027)))\n\ncustopfSpShDf \u003d custopfSpShDf.select(col(\u0027KUNNR_PK\u0027).alias(\u0027SoldToPoint\u0027),\u0027KUNN2_PK\u0027,\u0027PARVW_PK\u0027)\n\nsoldToCountDf \u003d custopfSpShDf.groupBy(\"SoldToPoint\",\"KUNN2_PK\").agg(countDistinct(\"PARVW_PK\").alias(\u0027count\u0027))\\\n.select(col(\u0027SoldToPoint\u0027).alias(\u0027SoldToPoint2\u0027),col(\u0027KUNN2_PK\u0027).alias(\u0027KUNN2_PK2\u0027),\u0027count\u0027)\n\nsoldToCount1Df \u003d custopfSpShDf.join(soldToCountDf,\\\n                                  (custopfSpShDf.SoldToPoint \u003d\u003d soldToCountDf.SoldToPoint2) \\\n                                  \u0026 (custopfSpShDf.KUNN2_PK \u003d\u003d soldToCountDf.KUNN2_PK2) \u0026 (soldToCountDf[\u0027count\u0027]\u003d\u003d1))\n\nsoldToCount2Df \u003d custopfSpShDf.join(soldToCountDf,\\\n                                  (custopfSpShDf.SoldToPoint \u003d\u003d soldToCountDf.SoldToPoint2) \u0026 (custopfSpShDf.KUNN2_PK \u003d\u003d soldToCountDf.KUNN2_PK2)\\\n                                  \u0026 (soldToCountDf[\u0027count\u0027]\u003d\u003d2) \u0026 (custopfSpShDf.PARVW_PK\u003d\u003d\u0027SH\u0027))\n\nsoldToDf \u003d soldToCount1Df.union(soldToCount2Df).drop(\u0027SoldToPoint2\u0027,\u0027KUNN2_PK2\u0027,\u0027PARVW_PK\u0027,\u0027count\u0027)\n\nsoldToTextDf \u003d soldToDf.join(customerFilteredDf,\\\n                           soldToDf.SoldToPoint\u003d\u003dcustomerFilteredDf[\u0027KUNNR_PK\u0027],how\u003d\u0027left\u0027).withColumnRenamed(\u0027TXTMD\u0027,\u0027SP_Description\u0027)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:54.814",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183835_565876609",
      "id": "20220822-102308_483740145",
      "dateCreated": "2022-09-12 11:29:43.835",
      "dateStarted": "2022-09-13 12:09:54.818",
      "dateFinished": "2022-09-13 12:09:55.130",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\nhydratePlusShiptoAndSoldToDf \u003d hydratePlusShiptoDf.join(soldToTextDf,\\\n                                                      hydratePlusShiptoDf[\u00270CUST_SALES\u0027]\u003d\u003dsoldToTextDf.KUNN2_PK,how\u003d\u0027left\u0027).drop(\u0027KUNN2_PK\u0027)\n                                                      \nhydratePlusShiptoAndSoldToDf \u003d hydratePlusShiptoAndSoldToDf.withColumn(\"0CUST_SALES\",\\\n                                                                       hydratePlusShiptoAndSoldToDf[\"0CUST_SALES\"].cast(IntegerType()))\n\nhydratePlusShiptoAndSoldToDf \u003d hydratePlusShiptoAndSoldToDf.\\\n    select(\u00270CUST_SALES\u0027,\u00270DISTR_CHAN\u0027,\u0027SoldToPoint\u0027,\u0027ZCUSTHG03___T\u0027,\u0027SP_Description\u0027,\u0027Active_From\u0027,\u0027Active_Till\u0027)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:09:55.218",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183835_261243524",
      "id": "20220822-102308_1450001682",
      "dateCreated": "2022-09-12 11:29:43.835",
      "dateStarted": "2022-09-13 12:09:55.222",
      "dateFinished": "2022-09-13 12:09:55.485",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\ncurrentDate \u003d datetime.today()\nhydratePlusShiptoAndSoldToDf \u003d hydratePlusShiptoAndSoldToDf.where( col(\u0027SoldToPoint\u0027).isNotNull() ).dropDuplicates()\n\nshipToMappingDf \u003d hydratePlusShiptoAndSoldToDf.where( (col(\u0027Active_From\u0027) \u003c\u003d currentDate) \u0026 (col(\u0027Active_Till\u0027) \u003e\u003d currentDate) ).\\\n    withColumnRenamed(\u0027ZCUSTHG03___T\u0027,\u0027GRP_DESC\u0027).\\\n    withColumnRenamed(\u00270DISTR_CHAN\u0027,\u0027Channel\u0027)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:10:23.061",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183835_1904166452",
      "id": "20220822-102308_431813570",
      "dateCreated": "2022-09-12 11:29:43.835",
      "dateStarted": "2022-09-13 12:10:23.065",
      "dateFinished": "2022-09-13 12:10:23.327",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\nshipToMappingDf\\\n.write.parquet(RESULT_MAPPING_PARQ,\nmode\u003d\"overwrite\"\n)",
      "user": "anonymous",
      "dateUpdated": "2022-09-13 12:10:59.723",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-2244-153a19905afb\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mshipToMappingDf\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m .write.parquet(RESULT_MAPPING_PARQ,\n\u001b[1;32m      3\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\n\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 939\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1304\u001b[0;31m         return_value \u003d get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003d\u003d\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o17931.parquet.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user\u003dzeppelin, access\u003dALL, inode\u003d\"/JUPITER/PROCESS/SHIPTO_TO_CLIENT_MAPPING/SHIPTO_TO_CLIENT_MAPPING.PARQUET\":dataproc-agent:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkSubAccess(FSPermissionChecker.java:348)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1896)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3104)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1127)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:708)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1614)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:949)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:946)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:956)\n\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:124)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:129)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:847)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user\u003dzeppelin, access\u003dALL, inode\u003d\"/JUPITER/PROCESS/SHIPTO_TO_CLIENT_MAPPING/SHIPTO_TO_CLIENT_MAPPING.PARQUET\":dataproc-agent:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkSubAccess(FSPermissionChecker.java:348)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1896)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3104)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1127)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:708)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1562)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1508)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1405)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n\tat com.sun.proxy.$Proxy17.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:644)\n\tat sun.reflect.GeneratedMethodAccessor140.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy18.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1612)\n\t... 39 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1662982183835_1847471009",
      "id": "paragraph_1662355412054_1486450090",
      "dateCreated": "2022-09-12 11:29:43.835",
      "dateStarted": "2022-09-13 12:10:59.727",
      "dateFinished": "2022-09-13 12:11:00.678",
      "status": "ERROR"
    }
  ],
  "name": "SHIPTO_TO_CLIENT_MAPPING",
  "id": "2HCACGM7U",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {
    "isRunning": false
  }
}